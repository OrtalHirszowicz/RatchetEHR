{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder Num:  \n",
      "Executed 1 SQLs\n",
      "Database already exists, set replace=True to rebuild\n",
      "Table not found in schema \"mimiciv_bsi_100_4h_test\", regenerating\n",
      "Regenerated Cohort in 1.5246467590332031 seconds\n",
      "Precentage of 1 label:  0.044470450555880635\n",
      "For now uses Weighted random sampler\n",
      "Precentage of 1 label after cleaning:  0.05150214592274678\n",
      "For now uses Weighted random sampler\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# %%\n",
    "# %%\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from os.path import dirname, abspath\n",
    "d = dirname(abspath(Path.cwd()))\n",
    "sys.path.insert(0, d)\n",
    "\n",
    "# %%\n",
    "# %%\n",
    "# %%\n",
    "# %%\n",
    "#bsi_tl_recon*\n",
    "# %% [markdown]\n",
    "# ## Imports and GPU setup\n",
    "\n",
    "# %%\n",
    "from copy import deepcopy\n",
    "from Utils.SkorchUtils.datasets import MyDatasetSingle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import Utils.dbutils as dbutils\n",
    "from Utils.feature_set_info import FeatureSetInfo\n",
    "import Generators.CohortGenerator as CohortGenerator\n",
    "import Generators.FeatureGenerator as FeatureGenerator\n",
    "import config\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "from skorch.exceptions import SkorchWarning\n",
    "warnings.filterwarnings('ignore', category=SkorchWarning)\n",
    "\n",
    "from Utils.data_utils import get_data\n",
    "import config\n",
    "\n",
    "from Utils.experiments_utils import ExperimentConducterTransferLearning\n",
    "\n",
    "from Utils.data_getter import DataGetter\n",
    "from Utils.model_params_init import get_model_params\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# from torch.optim import Optimizer\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "import hyper_params\n",
    "\n",
    "import math\n",
    "\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import random\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "\n",
    "# %%\n",
    "assert(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(hyper_params.DEVICE_NUM)\n",
    "\n",
    "print(\"Folder Num: \", hyper_params.ADDITIONAL_NAME)\n",
    "\n",
    "\n",
    "##%% \n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "# %% [markdown]\n",
    "# Some parameters:\n",
    "\n",
    "# %%\n",
    "SHOULD_USE_CACHE = False\n",
    "NUM_MESUREMENTS = 100 #00 #100 #600\n",
    "SHOULD_UPLOAD_SAVED_FEATURESET_INFO = True\n",
    "SHOULD_DO_GRID_SEARCH = False\n",
    "embedding_dim = 300 # size of embedding, must be multiple of number of heads\n",
    "NUM_HOURS_FOR_WINDOW = hyper_params.NUM_HOURS_FOR_WINDOW\n",
    "MODEL_NAME = \"Transformer\"\n",
    "ADDITIONAL_NAME_FOR_EXPERIMENT = hyper_params.CURR_TASK + (\"\" if hyper_params.CONDUCT_TRANSFER_LEARNING else '_no_transfer') + (\"\" if not hyper_params.USE_RNN else \"_rnn\") + (\"\" if not hyper_params.USE_LSTM else \"_lstm\") + \\\n",
    "    hyper_params.ADDITIONAL_NAME\n",
    "NUM_MOST_IMPORTANT_FEATURES = -1 #600 #587\n",
    "SHOULD_USE_WEIGHTS = False\n",
    "NUM_EXPERIMENTS = hyper_params.NUM_EXPERIMENTS\n",
    "TASK = 'mimiciv_' + hyper_params.CURR_TASK + '_' + str(NUM_MESUREMENTS) + '_' +str(NUM_HOURS_FOR_WINDOW) +'h'\n",
    "CONVERT_TWO = {10: '_19', 20: '_3_20', 100: '_100_2_1', 200: '_100_1', 1:'_1_1'}\n",
    "MORTALITY_TRANSFORMER_INIT_WEGITHS_LOCATION = None\n",
    "FEATURESET_FILE_NAME = None\n",
    "HIDDEN_SIZE = {100: 1, 20: 1, 10: 2, 1:1}\n",
    "NUM_ATTENTION_HEADS = HIDDEN_SIZE[NUM_MESUREMENTS]\n",
    "SHOULD_UPDATE_DATA = not SHOULD_UPLOAD_SAVED_FEATURESET_INFO\n",
    "\n",
    "PERFORM_RANDOM_SEARCH = False\n",
    "\n",
    "DF_PRECENTAGE = hyper_params.DF_PRECENTAGE \n",
    "# %% [markdown]\n",
    "# ## Cohort, Outcome and Feature Collection\n",
    "# \n",
    "# ### 1. Set up a connection to the OMOP CDM database\n",
    "# \n",
    "# Parameters for connection to be specified in ./config.py\n",
    "\n",
    "# %%\n",
    "schema_name = '\"' + TASK  + '_test\"' # all created tables will be created using this schema\n",
    "cohort_name = '\"' + '__' + TASK + '_cohort\"'\n",
    "reset_schema = False # if true, rebuild all data from scratch\n",
    "\n",
    "# %%\n",
    "# database connection parameters\n",
    "database_name = config.DB_NAME\n",
    "config_path = 'postgresql://{database_name}'.format(\n",
    "    database_name = database_name\n",
    ")\n",
    "connect_args = {\"host\": '/var/run/postgresql/', 'user': config.PG_USERNAME, \n",
    "                'password': config.PG_PASSWORD, 'database': config.DB_NAME} # connect_args to pass to sqlalchemy create_engine function\n",
    "\n",
    "# schemas \n",
    "cdm_schema_name = config.OMOP_CDM_SCHEMA # the name of the schema housing your OMOP CDM tables\n",
    "\n",
    "\n",
    "# set up database, reset schemas as needed\n",
    "db = dbutils.Database(config_path, schema_name, connect_args, cdm_schema_name)\n",
    "if reset_schema:\n",
    "    db.execute(\n",
    "        'drop schema if exists {} cascade'.format(schema_name)\n",
    "    )\n",
    "db.execute(\n",
    "    'create schema if not exists {}'.format(schema_name)\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Generate the Cohort as per the given SQL file\n",
    "\n",
    "# %%\n",
    "cohort_script_path = config.SQL_PATH_COHORTS + '/' + hyper_params.CURR_TASK + '_mimiciv_cohort.sql'\n",
    "\n",
    "# cohort parameters  \n",
    "params = {\n",
    "          'cohort_table_name'     : cohort_name,\n",
    "          'schema_name'           : schema_name,\n",
    "          'aux_data_schema'       : config.CDM_AUX_SCHEMA,\n",
    "          'min_hours_in_ICU'      : 48\n",
    "         }\n",
    "\n",
    "cohort = CohortGenerator.Cohort(\n",
    "    schema_name=schema_name,\n",
    "    cohort_table_name=cohort_name,\n",
    "    cohort_generation_script=cohort_script_path,\n",
    "    cohort_generation_kwargs=params,\n",
    "    outcome_col_name='y'\n",
    ")\n",
    "cohort.use_last_years =True\n",
    "cohort.build(db, replace=reset_schema)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Build the Feature Set by executing SQL queries and reading into tensors\n",
    "# The tensors are located in featureSet.tensors_for_person. A dictionary where each key is a person_id and each value is \n",
    "# the person's tensor.\n",
    "feature_set_path = config.DEFAULT_SAVE_LOC + '/featureset_' + TASK + '_' + str(NUM_MESUREMENTS) + '_' + str(NUM_HOURS_FOR_WINDOW) \\\n",
    "                    + '_' + MODEL_NAME + 'mimiciv'\n",
    "cache_data_path = config.DEFAULT_SAVE_LOC + '/cache_data_bsi_test_' + str(NUM_MESUREMENTS) + 'mimiciv'\n",
    "\n",
    "if SHOULD_UPLOAD_SAVED_FEATURESET_INFO and os.path.isfile(feature_set_path):\n",
    "    with open(feature_set_path, 'rb') as pickle_file:\n",
    "        featureSetInfo = pickle.load(pickle_file)\n",
    "else:\n",
    "    featureSet = FeatureGenerator.FeatureSet(db, task_name = TASK,\n",
    "    feature_set_file_name = FEATURESET_FILE_NAME)\n",
    "    temporal_features_list_observation = [{\"name\": 'language', \"observation_concept_id\": 40758030}]\n",
    "    featureSet.add_default_features(\n",
    "        [],\n",
    "        schema_name,\n",
    "        cohort_name,\n",
    "        from_sql_file = False,\n",
    "        type = \"Measurement\"\n",
    "    )\n",
    "    eicu_measurements = ['measurements_mimiciv']\n",
    "    featureSet.add_default_features(\n",
    "        eicu_measurements,\n",
    "        schema_name,\n",
    "        cohort_name,\n",
    "        from_sql_file = True,\n",
    "        type = \"Measurement\"\n",
    "    )\n",
    "    non_temportal_feature_list = ['age_mimiciv',  'gender_mimiciv',  'first_care_unit_mimiciv']\n",
    "    featureSet.add_default_features(\n",
    "        non_temportal_feature_list,\n",
    "        schema_name,\n",
    "        cohort_name,\n",
    "        temporal = False,\n",
    "        type = \"General\"\n",
    "    )\n",
    "\n",
    "    non_temportal_feature_list = ['diagnosis_mimiciv', 'medical_history_mimiciv', 'drug_mimiciv']# 'procedures', 'drug']\n",
    "    featureSet.add_default_features(\n",
    "        non_temportal_feature_list,\n",
    "        schema_name,\n",
    "        cohort_name,\n",
    "        temporal = True,\n",
    "        type = \"Diagnosis\",\n",
    "        with_feature_end_date = True\n",
    "    )\n",
    "    time_delta = FeatureGenerator.TimeDelta(hours = NUM_HOURS_FOR_WINDOW)#(hours = 2)\n",
    "    #numeric_names = pd.read_csv(\"./Tables/mimic_name_to_general.txt\", sep = ' -- ')\n",
    "    #featureSet.numeric_features += list(numeric_names[\"General\"].values)\n",
    "    # featureSet.postprocess_func = post_process\n",
    "    featureSet.build(cohort, time_delta = time_delta, from_cached=SHOULD_USE_CACHE, cache_file=cache_data_path,\n",
    "                    use_prebuilt_features = False)\n",
    "    featureSet.build_bit_vec_features(cohort, time_delta = time_delta, from_cached=SHOULD_USE_CACHE, cache_file=cache_data_path,\n",
    "                    use_prebuilt_features = False)              \n",
    "    featureSetInfo = FeatureSetInfo(featureSet, task_name=TASK)\n",
    "    with open(feature_set_path, 'wb') as pickle_file:\n",
    "        pickle.dump(featureSetInfo, pickle_file)\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Process the collected data and calculate indices needed for the deep model\n",
    "\n",
    "# %%\n",
    "def get_dict_path(person_id):\n",
    "    return config.DEFAULT_SAVE_LOC + '/Dictionary' + TASK +'/' + str(person_id)\n",
    "\n",
    "# %%\n",
    "person_indices  = featureSetInfo.person_ids\n",
    "orig_person_indices = list(map(int, person_indices))\n",
    "unique_id = featureSetInfo.unique_id_col\n",
    "person_indices = set(orig_person_indices).intersection(set(cohort._cohort[unique_id].values))\n",
    "person_indices = [(0, x) for x in person_indices]\n",
    "idx_to_person = {i : id for i, id in enumerate(sorted(list(person_indices)))}\n",
    "person_to_idx = {id : i for i, id in enumerate(sorted(list(person_indices)))}\n",
    "#visits_data = featureSetInfo.post_process_transformer\n",
    "dict_file = get_dict_path(person_indices[0][1]) + '_postprocessed_tensor'\n",
    "with open(dict_file, 'rb') as pickle_file:\n",
    "    curr_tensor_info = pickle.load(pickle_file)\n",
    "    max_visits = curr_tensor_info[0].shape[0]\n",
    "n_visits = {int(p): math.ceil(np.max(np.array(featureSetInfo.window_times_for_person[int(p)])).astype('timedelta64[h]').astype(int) / NUM_HOURS_FOR_WINDOW) for p in featureSetInfo.person_ids }\n",
    "if hyper_params.USE_INIT_DATA:\n",
    "    person_indices = [(0, p) for p, val in n_visits.items() if val > (hyper_params.NUM_HOURS_PREDICTION // NUM_HOURS_FOR_WINDOW)]\n",
    "person_indices = sorted(person_indices, key = lambda x: x[1])\n",
    "curr_cohort = cohort._cohort[np.isin(cohort._cohort[unique_id].values, [x[1] for x in person_indices])]\n",
    "curr_cohort = curr_cohort.sort_values(by = unique_id)\n",
    "outcomes_filt = curr_cohort['y'].values\n",
    "if hyper_params.USE_TEST_GROUP:\n",
    "    is_last_years = curr_cohort['last_years'].values\n",
    "# %%\n",
    "one_label_precentage = np.sum(outcomes_filt) / len(outcomes_filt)\n",
    "print(\"Precentage of 1 label: \", one_label_precentage)\n",
    "print(\"For now uses Weighted random sampler\")\n",
    "\n",
    "\n",
    "# %%\n",
    "if SHOULD_UPDATE_DATA:\n",
    "    def update_data(person_id):\n",
    "        dict_file = get_dict_path(person_id) + \"_postprocessed_tensor\"\n",
    "        with open(dict_file, 'rb') as pickle_file:\n",
    "            val = pickle.load(pickle_file)\n",
    "        dict_file = get_dict_path(person_id) + '_transformer'\n",
    "        with open(dict_file, 'wb') as pickle_file:\n",
    "            pickle.dump((val[0], val[1].to_dense(), val[2]), pickle_file)\n",
    "        \n",
    "    [update_data(person_id[1]) for person_id in person_indices]\n",
    "\n",
    "# %%\n",
    "### load the data from memory into visits_data\n",
    "\n",
    "\n",
    "# def get_data_transformer(person_id):\n",
    "#     dict_file = get_dict_path(person_id) + '_postprocessed_tensor'\n",
    "#     with open(dict_file, 'rb') as pickle_file:\n",
    "#         val = pickle.load(pickle_file)\n",
    "#     return (val[0].to_dense(), val[1], val[2])\n",
    "\n",
    "# source_visits_data = OrderedDict({person_id: get_data_transformer(person_id) for person_id in sorted(person_indices)})\n",
    "\n",
    "#TODO: Ortal - add this\n",
    "\n",
    "visits_data = DataGetter([TASK])\n",
    "\n",
    " # %%\n",
    "dataset_dict = {\n",
    "    'person_indices': person_indices, #The ids of all the persons\n",
    "    'outcomes_filt': outcomes_filt, # A pandas Series defined such that outcomes_filt.iloc[i] is the outcome of the ith patient\n",
    "    'idx_to_person': idx_to_person,\n",
    "    'n_visits': n_visits,\n",
    "    'visits_data': visits_data,\n",
    "    'num_invariant_features': featureSetInfo.num_non_numeric_features,\n",
    "}\n",
    "\n",
    "\n",
    "not_good_features_file_path = config.DEFAULT_SAVE_LOC + 'not_good_features_file_path_mimiciv'\n",
    "with open(not_good_features_file_path, 'rb') as pickle_file:\n",
    "    not_good_features = pickle.load(pickle_file)\n",
    "    num_numeric_features = len(featureSetInfo.numeric_feature_to_index)\n",
    "    good_features = list(set(range(num_numeric_features)).difference(not_good_features))\n",
    "    dataset_dict['not_good_features'] = not_good_features\n",
    "\n",
    "# %%\n",
    "#Filtering out samples without enough data:\n",
    "if hyper_params.CURR_TASK != 'mortality':\n",
    "    total_n_visits = deepcopy(n_visits)\n",
    "    dataset = MyDatasetSingle(hyper_params.MAX_VISITS, total_n_visits, dataset_dict['visits_data'], TASK, person_indices, y = None,\n",
    "    mbsz = hyper_params.MBSZ, dataset_dict = dataset_dict, feature_set_info=featureSetInfo, should_mask_input= False,\n",
    "    if_clean_data= False)\n",
    "    features_info_counter = None\n",
    "    num_numeric_features = len(featureSetInfo.numeric_feature_to_index)\n",
    "    info_precentages = {}\n",
    "\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "    for x in DataLoader(worker_init_fn = seed_worker, generator = g, dataset = dataset, batch_size = hyper_params.MBSZ, pin_memory=True, num_workers=hyper_params.NUM_WORKERS):\n",
    "        curr_person_indices = x[0][0][1].tolist()\n",
    "        curr_info = x[0][1]\n",
    "        for i in range(curr_info.shape[0]):\n",
    "            curr_person_idx = curr_person_indices[i]\n",
    "            curr_info_person = curr_info[i, :total_n_visits[curr_person_idx], :]\n",
    "            if curr_info_person.shape[0] * curr_info_person.shape[1] != 0: \n",
    "                info_precentages[curr_person_idx] = torch.sum(1 * (curr_info_person != 0)).item() / (curr_info_person.shape[0] * curr_info_person.shape[1])\n",
    "            else:\n",
    "                info_precentages[curr_person_idx] = 0\n",
    "    dataset_dict['info_precentages'] = info_precentages\n",
    "    dataset_dict['weight'] = dataset_dict['info_precentages']\n",
    "    not_relevant_person_indices = [p_id for p_id, p in info_precentages.items() if p <= hyper_params.STAY_INFO_PRECENTAGE_LOW_THRESHOLD]\n",
    "    not_relevant_person_indices_eICU = set([(0, p) for p in not_relevant_person_indices]).intersection(set(person_indices))\n",
    "    # not_relevant_person_indices_val_test = [p_id for p_id, p in info_precentages.items() if p < hyper_params.STAY_INFO_PRECENTAGE_LOW_THRESHOLD_VAL_TEST]\n",
    "    # not_relevant_person_indices_val_test = set([(0, p) for p in not_relevant_person_indices_val_test]).intersection(set(person_indices))\n",
    "    #not_relevant_person_indices_eICU = not_relevant_person_indices_eICU.union(not_relevant_person_indices_val_test)\n",
    "    not_relevant_person_indices_eICU = [person_indices.index(p) for p in not_relevant_person_indices_eICU]\n",
    "    person_indices = [(a, b) for a, b in list(np.delete(np.array(person_indices), not_relevant_person_indices_eICU, axis = 0))]\n",
    "    outcomes_filt = list(np.delete(np.array(outcomes_filt), not_relevant_person_indices_eICU))\n",
    "    dataset_dict['person_indices'] = person_indices\n",
    "    dataset_dict['outcomes_filt'] = outcomes_filt\n",
    "    if hyper_params.USE_TEST_GROUP:\n",
    "        is_last_years = list(np.delete(np.array(is_last_years), not_relevant_person_indices_eICU))\n",
    "        dataset_dict['is_last_years'] = is_last_years\n",
    "\n",
    "# %%\n",
    "one_label_precentage = np.sum(outcomes_filt) / len(outcomes_filt)\n",
    "print(\"Precentage of 1 label after cleaning: \", one_label_precentage)\n",
    "print(\"For now uses Weighted random sampler\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# split data into train, validate and test sets\n",
    "validation_precentage = 0.5\n",
    "test_val_precentage = hyper_params.TEST_VAL_PRECENTAGE\n",
    "if 1-DF_PRECENTAGE > 0:\n",
    "    person_indices, _ = train_test_split(sorted(dataset_dict['person_indices'], key = lambda x: x[1]), test_size = (1-DF_PRECENTAGE), \n",
    "    stratify=outcomes_filt)\n",
    "    dataset_dict['person_indices'] =  person_indices\n",
    "# %%\n",
    "### Some default parameters:\n",
    "ft_epochs = hyper_params.FT_EPOCHS #508\n",
    "#ft_epochs = ft_epochs * int ((1-test_val_precentage) * len(person_indices)) #800 #* 2 #400\n",
    "#print(ft_epochs)\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# using the same split as before, create train/validate/test batches for the deep model\n",
    "# `mbsz` might need to be decreased based on the GPU's memory and the number of features being used\n",
    "mbsz = hyper_params.MBSZ #64 #64 * 2\n",
    "# Pick a name for the model (mn_prefix) that will be used when saving checkpoints\n",
    "# Then, set some parameters for SARD. The values below reflect a good starting point that performed well on several tasks\n",
    "mn_prefix = 'bsi_experiment_prefix'\n",
    "n_heads = NUM_ATTENTION_HEADS #2\n",
    "#assert embedding_dim % n_heads == 0\n",
    "\n",
    "print(len(featureSetInfo.numeric_feature_to_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, new_dataset_dict = \\\n",
    "        get_data(visits_data, dataset_dict['person_indices'], dataset_dict, \n",
    "        test_val_precentage, validation_precentage, \n",
    "            max_visits, n_visits, curr_cohort, fix_imbalance = False, need_to_clean_data = False, featureSetInfo = featureSetInfo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAKE_MEDIAN = True\n",
    "TAKE_LAST = False\n",
    "\n",
    "total_n_visits = deepcopy(n_visits)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(X, y, is_train = False):\n",
    "    dataset = MyDatasetSingle(max_visits, total_n_visits, dataset_dict['visits_data'], TASK, X, y, \n",
    "                clf = None, mbsz = mbsz, dataset_dict = dataset_dict, feature_set_info=featureSetInfo, \n",
    "                should_mask_input=False)\n",
    "    num_samples = 128\n",
    "\n",
    "    dataset = DataLoader(dataset, pin_memory = True, batch_size=num_samples, num_workers = hyper_params.NUM_WORKERS)\n",
    "    y = []\n",
    "    all_data = []\n",
    "    person_indices = []\n",
    "    for t in dataset:\n",
    "        if TAKE_MEDIAN:\n",
    "            all_data.append(torch.cat([torch.median(t[0][1][i, min(hyper_params.MAX_VISITS - 8, max(0, total_n_visits[p.item()] - 8)) :total_n_visits[p.item()] + 1, :], dim = 0).values.unsqueeze(0) for i, p in enumerate(t[0][0]) if total_n_visits[p.item()] > 0]))\n",
    "        elif TAKE_LAST:\n",
    "            all_data.append(torch.cat([t[0][1][i, min(total_n_visits[p.item()] - 1, hyper_params.MAX_VISITS - 1), :].unsqueeze(0) for i, p in enumerate(t[0][0]) if total_n_visits[p.item()] > 0]))\n",
    "        curr_y = list(t[1].detach().cpu().numpy())\n",
    "        curr_person_indices = list(t[0][0].detach().cpu().numpy())\n",
    "        y += [x for x, p in zip(curr_y, curr_person_indices) if total_n_visits[p] > 0]\n",
    "        person_indices += [p for p in person_indices if total_n_visits[p] > 0]\n",
    "    new_X = torch.cat(all_data, dim = 0).detach().cpu().numpy()\n",
    "    new_y = y\n",
    "    return new_X, new_y\n",
    "\n",
    "# new_X_train, new_y_train = build_dataset(X_val + X_train, y_val + y_train, True)\n",
    "# new_X_test, new_y_test = build_dataset(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from xgboost import XGBClassifier\n",
    "xgboost = XGBClassifier()\n",
    "params = {\n",
    "    \"max_depth\": [i for i in range(2, 11)], \n",
    "    \"booster\": [\"dart\"],\n",
    "    \"eta\": [0.01, 0.02, 0.005], #,0.001, 1e-3],\n",
    "    \"gamma\": [0.1, 0.01, 0.001, 1, 2, 10],\n",
    "    \"lambda\": [0.01, 10, 1, 0.1,],\n",
    "    \"alpha\": [0.01, 10, 1, 0.1,],\n",
    "    \"grow_policy\": [\"depthwise\", \"lossguide\"],\n",
    "    \"max_leaves\": [5, 10, 50, 100],\n",
    "    'subsample': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "clf = RandomizedSearchCV(xgboost, params, scoring = 'roc_auc', verbose = 3, n_iter = 10000, n_jobs = 8)\n",
    "if PERFORM_RANDOM_SEARCH:\n",
    "    new_X_train, new_y_train = build_dataset(X_val + X_train, y_val + y_train, True)\n",
    "    new_X_test, new_y_test = build_dataset(X_test, y_test)\n",
    "    clf.fit(new_X_train, new_y_train)\n",
    "    print(\"################################################\")\n",
    "    print(\"ROC-AUC Score: \", roc_auc_score(new_y_test, clf.predict_proba(new_X_test)[:, 1]))\n",
    "    print(\"Best params:\", clf.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "auc_pr_scores = []\n",
    "for i in range(NUM_EXPERIMENTS):\n",
    "    #Done using 10,000 random searches\n",
    "    params = {\n",
    "        \"alpha\": 0.01, \"booster\": \"dart\", \"eta\": 0.005, \"gamma\": 2, \"grow_policy\": \"depthwise\", \n",
    "    \"lambda\": 0.01, \"max_depth\": 7, \"max_leaves\": 5, \"subsample\": 0.4\n",
    "    \n",
    "    }\n",
    "    clf = XGBClassifier(**params)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, new_dataset_dict = \\\n",
    "        get_data(visits_data, dataset_dict['person_indices'], dataset_dict,\n",
    "        test_val_precentage, validation_precentage, \n",
    "            max_visits, total_n_visits, curr_cohort, fix_imbalance = False, need_to_clean_data = False, featureSetInfo = featureSetInfo)\n",
    "    new_X_train, new_y_train = build_dataset(X_train + X_val, y_train + y_val, True)\n",
    "    new_X_test, new_y_test = build_dataset(X_test, y_test)\n",
    "    clf.fit(new_X_train, new_y_train)\n",
    "    print(clf.feature_importances_)\n",
    "    curr_score = roc_auc_score(new_y_test, clf.predict_proba(new_X_test)[:, 1])\n",
    "    curr_auc_pr_score = average_precision_score(new_y_test, clf.predict_proba(new_X_test)[:, 1])\n",
    "    print(\"ROC-AUC Score: \", curr_score)\n",
    "    scores.append(curr_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC ROC Score\")\n",
    "print(\"Mean: \", np.array(scores).mean())\n",
    "print(\"STD: \", np.array(scores).std())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1ea94ee601290ca92cde6b572b544f987186724d782a79f547c1d862f89ffe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
